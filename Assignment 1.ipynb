{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Notes: \n",
    "* Answer each question in a separate Jupynet Notebook Cell\n",
    "* Pleas keep the code in your cells short. \n",
    "  * In notebook programming cells are typicaly short to facilitate reading. \n",
    "  * If well toughout, most answers in this assignment won're require more than 3 or 4 lines of code. \n",
    "* Do no change the list of import, i.e., do not add additional libraries. Those included are the only ones you are allowed to use.\n",
    "* Add your first and Last name below:\n",
    "\n",
    "<Firt Name> <Last Name>\n",
    "    \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Samuel Birns\r\n",
    "UH ID: 25187488"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\r\n",
    "from itertools import product\r\n",
    "from collections import Counter\r\n",
    "# from tqdm.notebook import tqdm\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this assignment you will be working with Corona Virus (SARS-CoV2) data that was obtained from the [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/). You will need two files. The first (`data/coronavirus_info.csv`) is small and is provided in the GitHub Repo. The second  (`data_report.jsonl`) is larger so you will need to download a compressed version, which you will need to uncompress prior to using. You can downlod the second file here:\r\n",
    "\r\n",
    "https://www.dropbox.com/s/qdn67rshygz06ff/data_report.jsonl.gz?dl=0\r\n",
    "\r\n",
    "We start by loading `data/coronavirus_info.csv` (Code provided below)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "table = pd.read_csv(\"data/coronavirus_info.csv\", low_memory=False)\r\n",
    "table = table.drop([\"US State\", \"Host Name\", \"Host Taxonomy ID\", \"Sequence Type\", \"Species Taxonomy Id\", \"Nuc Completeness\", \"BioProject\", \"BioSample\"], axis=1)\r\n",
    "\r\n",
    "missing = table[\"Geo Location\"].isnull()\r\n",
    "table.loc[missing, \"Geo Location\"] = \"\"\r\n",
    "\r\n",
    "\r\n",
    "table.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nucleotide Accession</th>\n",
       "      <th>Species Name</th>\n",
       "      <th>Virus Genus</th>\n",
       "      <th>Virus Family</th>\n",
       "      <th>Isolate Name</th>\n",
       "      <th>Nucleotide Length</th>\n",
       "      <th>Geo Location</th>\n",
       "      <th>Collection Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_045512.2</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>Wuhan-Hu-1</td>\n",
       "      <td>29903</td>\n",
       "      <td>Asia; China</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK058807.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04825/2021</td>\n",
       "      <td>29801</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-07-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK058777.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04790/2021</td>\n",
       "      <td>29771</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OK058695.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04700/2021</td>\n",
       "      <td>29820</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OK058662.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04651/2021</td>\n",
       "      <td>29798</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OK058592.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/MA-MASPHL-04499/2021</td>\n",
       "      <td>29802</td>\n",
       "      <td>North America; USA</td>\n",
       "      <td>2021-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OK056996.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/FL-CDC-QDX27934346/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: Florida</td>\n",
       "      <td>2021-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OK056909.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/CA-CDC-QDX27909662/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: California</td>\n",
       "      <td>2021-08-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OK056850.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/FL-CDC-QDX27934406/2021</td>\n",
       "      <td>29763</td>\n",
       "      <td>North America; USA: Florida</td>\n",
       "      <td>2021-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OK056784.1</td>\n",
       "      <td>Severe acute respiratory syndrome coronavirus 2</td>\n",
       "      <td>Betacoronavirus</td>\n",
       "      <td>Coronaviridae</td>\n",
       "      <td>SARS-CoV-2/human/USA/NY-CDC-QDX28007789/2021</td>\n",
       "      <td>29775</td>\n",
       "      <td>North America; USA: New York</td>\n",
       "      <td>2021-08-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nucleotide Accession                                     Species Name  \\\n",
       "0          NC_045512.2  Severe acute respiratory syndrome coronavirus 2   \n",
       "1           OK058807.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "2           OK058777.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "3           OK058695.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "4           OK058662.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "5           OK058592.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "6           OK056996.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "7           OK056909.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "8           OK056850.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "9           OK056784.1  Severe acute respiratory syndrome coronavirus 2   \n",
       "\n",
       "       Virus Genus   Virus Family  \\\n",
       "0  Betacoronavirus  Coronaviridae   \n",
       "1  Betacoronavirus  Coronaviridae   \n",
       "2  Betacoronavirus  Coronaviridae   \n",
       "3  Betacoronavirus  Coronaviridae   \n",
       "4  Betacoronavirus  Coronaviridae   \n",
       "5  Betacoronavirus  Coronaviridae   \n",
       "6  Betacoronavirus  Coronaviridae   \n",
       "7  Betacoronavirus  Coronaviridae   \n",
       "8  Betacoronavirus  Coronaviridae   \n",
       "9  Betacoronavirus  Coronaviridae   \n",
       "\n",
       "                                   Isolate Name  Nucleotide Length  \\\n",
       "0                                    Wuhan-Hu-1              29903   \n",
       "1     SARS-CoV-2/human/USA/MA-MASPHL-04825/2021              29801   \n",
       "2     SARS-CoV-2/human/USA/MA-MASPHL-04790/2021              29771   \n",
       "3     SARS-CoV-2/human/USA/MA-MASPHL-04700/2021              29820   \n",
       "4     SARS-CoV-2/human/USA/MA-MASPHL-04651/2021              29798   \n",
       "5     SARS-CoV-2/human/USA/MA-MASPHL-04499/2021              29802   \n",
       "6  SARS-CoV-2/human/USA/FL-CDC-QDX27934346/2021              29775   \n",
       "7  SARS-CoV-2/human/USA/CA-CDC-QDX27909662/2021              29775   \n",
       "8  SARS-CoV-2/human/USA/FL-CDC-QDX27934406/2021              29763   \n",
       "9  SARS-CoV-2/human/USA/NY-CDC-QDX28007789/2021              29775   \n",
       "\n",
       "                     Geo Location Collection Date  \n",
       "0                     Asia; China         2019-12  \n",
       "1              North America; USA      2021-07-29  \n",
       "2              North America; USA      2021-08-10  \n",
       "3              North America; USA      2021-08-15  \n",
       "4              North America; USA      2021-08-09  \n",
       "5              North America; USA      2021-07-26  \n",
       "6     North America; USA: Florida      2021-08-18  \n",
       "7  North America; USA: California      2021-08-16  \n",
       "8     North America; USA: Florida      2021-08-18  \n",
       "9    North America; USA: New York      2021-08-21  "
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.1\n",
    "\n",
    "* The location of each of the sequences is recorded under the `Geo Location` column.  How many entries are from Asia?\n",
    "  * Note that for some records, the `Geo Location` column is missing\n",
    "  * Display the results using the following format: \n",
    "    Asia: XXXX,\n",
    "    North America': XXXX,\n",
    "    Europe: XXXX,\n",
    "    Oceania: XXXX,\n",
    "    Africa: XXXX,\n",
    "    South America: XXXX \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Write your code here\r\n",
    "asia_loc = table[table['Geo Location'].str.contains('Asia')]\r\n",
    "na_loc = table[table['Geo Location'].str.contains('North America')]\r\n",
    "europe_loc = table[table['Geo Location'].str.contains('Europe')]\r\n",
    "oceania_loc = table[table['Geo Location'].str.contains('Oceania')]\r\n",
    "africa_loc = table[table['Geo Location'].str.contains('Africa')]\r\n",
    "sa_loc = table[table['Geo Location'].str.contains('South America')]\r\n",
    "print('Asia:', len(asia_loc), ', North America:', len(na_loc), ', Europe:', len(europe_loc), ', Oceania:', len(oceania_loc), ', Africa:', len(africa_loc), ', South America:', len(sa_loc))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Asia: 3903 , North America: 224014 , Europe: 189100 , Oceania: 10301 , Africa: 1405 , South America: 534\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.2\n",
    "Use the `coronavirus_info.csv` table to count the entries that are from Hawaii. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Write your code here\r\n",
    "len(table[table['Geo Location'].str.contains('Hawaii')])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.3\n",
    "\n",
    "The file `data_report.jsonl` contains the variants of the virus in the DB. This `json` file is a list of records (one per line) for each one of the genomes in the database. Before we work with the large file, we will experiment with a file containing a single record.\n",
    "\n",
    "The file `single_record.json` contains a single sample record. Use the `JSON library to load the file `single_record.json` into a variable called `sample_vir_record`\n",
    "\n",
    "1. how many first-level keys does this record have?\n",
    "  * Do not count nested keys. Only those at the top level should be counted\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Write your code here\r\n",
    "import json\r\n",
    "f = open('data\\single_record.json')\r\n",
    "sample_vir_record = json.load(f)\r\n",
    "len(sample_vir_record.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.4\n",
    "\n",
    "Each Covid various in this database is classified according to a system referred to as the Pangolin (Phylogenetic Assignment of Named Global Outbreak LINeages) classification. It is not essential to complete the assignment that you understand this system, but if you're interested in learning more, see:\n",
    "\n",
    "https://cov-lineages.org/resources/pangolin.html\n",
    "\n",
    "The Pangolin classification of this sample record is nested within the `virus` key:\n",
    "```json\n",
    "{ ...\n",
    "  \"virus\": {\n",
    "              ...\n",
    "              \"pangolinClassification\": \n",
    "              ...\n",
    "            }\n",
    "  ... \n",
    "}\n",
    "```\n",
    "Write code to extract the classification of this record. The result should be `B.1.1.214`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Write your code here\r\n",
    "sample_vir_record['virus']['pangolinClassification']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'B.1.1.214'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We hear in the news about `Alpha`, `Beta`, `Delta` variants of concern and recently the Mu variant as being of interest. Basedon your answer to `Q.3`, you may have been tempted to infer that this virus is of type `Beta` since the first letter is `B`. In fact, this variant of type `Alpha` and is a variant of concern. Although not relevant to this exercise, the rules for naming new variants and the list of known `SARS-CoV-2` are provided here:\n",
    "\n",
    "https://www.pango.network/how-does-the-system-work/what-are-pango-lineages/\n",
    "\n",
    "https://cov-lineages.org/lineage_list.html\n",
    "\n",
    "\n",
    "The following short video is very helpful for understanding what a variant is, how it arises, how it's named, and why some variants are more concerning than others.\n",
    "\n",
    "https://www.youtube.com/watch?v=B8UEZ9cfgz4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.5\n",
    "\n",
    "Write Python code that counts all the different kinds of variants in `data_report.jsonl`. \n",
    "Because the file is 7GB, you won't likely be able to load it into your laptop's RAM using Python. I encountered this error when trying to open it on my laptop\n",
    "\n",
    "![](https://www.dropbox.com/s/lieo685pafkgm5e/ram_error.png?dl=1)\n",
    "\n",
    "\n",
    "It would be easy to extract the data from the pangolinClassification field of each `json` record by reading each line (a record) at a time.\n",
    "\n",
    "The list of current variants of concern we are interested in are:\n",
    "      * Alpha (B.1.1.7)\n",
    "      * Beta (B.1.351, B.1.351.2, B.1.351.3)\n",
    "      * Delta (B.1.617.2, AY.1, AY.2, AY.3)\n",
    "      * Gamma (P.1, P.1.1, P.1.2) \n",
    "\n",
    "You should get something similar to what follows:\n",
    "```\n",
    "Alpha: X\n",
    "Beta: X\n",
    "Delta: X\n",
    "Gamma: X\n",
    "```\n",
    "Where `X` represents the counts for relevant variants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Write your code here\r\n",
    "with open('data\\data_report.jsonl') as f:\r\n",
    "    records = []\r\n",
    "    variant_dict = {'Alpha':['B.1.1.7'], 'Beta': ['B.1.351', 'B.1.351.2', 'B.1.351.3'], 'Delta': ['B.1.617.2', 'AY.1', 'AY.2', 'AY.3'], 'Gamma': ['P.1', 'P.1.1', 'P.1.2']}\r\n",
    "    for record in f:\r\n",
    "        record = json.loads(record)\r\n",
    "        try:\r\n",
    "            vir_record = record['virus']['pangolinClassification']\r\n",
    "            for key, value in variant_dict.items():\r\n",
    "                for item in value:\r\n",
    "                    if vir_record == item:\r\n",
    "                        records.append(key)\r\n",
    "        except KeyError:\r\n",
    "            continue\r\n",
    "    count = Counter(records)\r\n",
    "    print(count)\r\n",
    "f.close()     "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5e822e293d1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvariant_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Alpha'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'B.1.1.7'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Beta'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'B.1.351'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B.1.351.2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B.1.351.3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Delta'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'B.1.617.2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AY.1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AY.2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'AY.3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Gamma'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'P.1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P.1.1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P.1.2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find similar viruses.\n",
    "\n",
    "It's often useful to compare viruses to study how similar strains are. While sophisticated algorithms to compare a pair of viruses exist, these are typically computationally intensive and cannot be used to carry out a large number of comparisons. \n",
    "\n",
    "An alternative, albeit less sensitive, approach consists of comparing word counts (called k-mers, where k is the word size) across genomes.  Suppose we have two viruses X and Y, with the following Genomes.\n",
    "```\n",
    "X = \"ACGTAGTGCATGTGTAGCTGTGTAGCTGTAC\"\n",
    "Y = \"ACTAGTGCATGTGTAGCTCTGTAGCTGATAC\"\n",
    "```\n",
    "\n",
    "To compare `X` and `Y`, we first vectorize these genomes by marking the presence of words (k-mers) as a boolean value, 0 if absent and 1 if the word is present. This method assumes that similar genomes will have the same words, which makes sense.\n",
    "\n",
    "This idea, which is referred to as the bag of words model is computationally efficient, making it ideal to vectorize text in big data analytics. Another variant of this model requires replacing the presence and absence by counts for each word.\n",
    "\n",
    "The code below vectorizes an input DNA sequence intro k-mers of size k=2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "def get_kmer_2(X):\r\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "    words_size_2 = [\"\".join(dna_prod) for dna_prod in product(DNA, DNA)]\r\n",
    "    counts = pd.Series([0 for _ in words_size_2], index = words_size_2)\r\n",
    "    words_in_X = set([X[i:i+2] for i in range(0, len(X)-1)])\r\n",
    "    counts[list(words_in_X)] = 1\r\n",
    "    return counts    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "# X has 3 words of size 2 (AC, CG, GT)\r\n",
    "X = \"ACGT\"\r\n",
    "get_kmer_2(\"ACGT\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AA    0\n",
       "AC    1\n",
       "AG    0\n",
       "AT    0\n",
       "CA    0\n",
       "CC    0\n",
       "CG    1\n",
       "CT    0\n",
       "GA    0\n",
       "GC    0\n",
       "GG    0\n",
       "GT    1\n",
       "TA    0\n",
       "TC    0\n",
       "TG    0\n",
       "TT    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function below takes a dictionary of sequences' counts as a `pandas Series` and prints it using HTML Table, which you might agree is nicer to visualize."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "def pretty_print_counts(counts_dict):\r\n",
    "    list_of_count = [data.to_list() for data in counts_dict.values()]\r\n",
    "    list_of_indices = [x for x in counts_dict.keys()]\r\n",
    "    list_of_columns = list(counts_dict.values())[0].index.to_list()\r\n",
    "    df_single_level_cols = pd.DataFrame(list_of_count,\r\n",
    "                                        index=[x for x in counts_dict.keys()],\r\n",
    "                                       columns = list_of_columns)    \r\n",
    "    return df_single_level_cols "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "\r\n",
    "X = \"ACGTACGTACGTACGT\"\r\n",
    "Y = \"ACGTACAAACGTACGT\"\r\n",
    "Z = \"TTTTACAAACGTTTTT\"\r\n",
    "\r\n",
    "counts_dict = {\"X\": get_kmer_2(X), \"Y\": get_kmer_2(Y), \"Z\": get_kmer_2(Z)}\r\n",
    "pretty_print_counts(counts_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   1   0   0   0   0   1   0   0   0   0   1   1   0   0   0\n",
       "Y   1   1   0   0   1   0   1   0   0   0   0   1   1   0   0   0\n",
       "Z   1   1   0   0   1   0   1   0   0   0   0   1   1   0   0   1"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.6\r\n",
    "\r\n",
    "Write a function that computes the Jaccard similarity between two feature vectors, A and B. If you recall, Jaccard similarity is computed as:\r\n",
    "\r\n",
    "$$J(A,B) = \\frac{A \\cap B}{A \\cup B}$$\r\n",
    "\r\n",
    "In other words, the number of items shared by `A` and `B` over the set of all items in `A` or `B`.\r\n",
    "\r\n",
    "For example, for `A= get_kmer_2(X)` and Y = get_kmer_2(B) above,\r\n",
    "\r\n",
    "$$\r\n",
    "J(A,B) = \\frac{4}{6}\r\n",
    "$$\r\n",
    "\r\n",
    "Your function should have the following signature: \r\n",
    "\r\n",
    "`jaccard(A, B)`\r\n",
    "\r\n",
    "Where `A` and `B` are `pandas Series`\r\n",
    "\r\n",
    "\r\n",
    "Test your function using the code below to make sure it's correct."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Write you code here\r\n",
    "def jaccard(A, B):\r\n",
    "    df = pd.DataFrame({'a': A, 'b': B})\r\n",
    "    df_intersection = df[(df['a'] == 1) & (df['b'] == 1)]\r\n",
    "    df_union = df[(df['a'] == 1) | (df['b'] == 1)]\r\n",
    "    return len(df_intersection) / len(df_union)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\r\n",
    "A = get_kmer_2(X)\r\n",
    "B = get_kmer_2(Y)\r\n",
    "assert jaccard(A, B) == 4/6"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.7 \n",
    "\n",
    "* Compute the jaccard similarity for the pairs of sequences `(X, Y)`, `(X, Z)`, `(Y, Z)`. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Write you code here\r\n",
    "print('Jaccard distance between X and Y: ', jaccard(get_kmer_2(X),get_kmer_2(Y)))\r\n",
    "print('Jaccard distance between X and Z: ', jaccard(get_kmer_2(X),get_kmer_2(Z)))\r\n",
    "print('Jaccard distance between Y and Z: ', jaccard(get_kmer_2(Y),get_kmer_2(Z)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jaccard distance between X and Y:  0.6666666666666666\n",
      "Jaccard distance between X and Z:  0.5714285714285714\n",
      "Jaccard distance between Y and Z:  0.8571428571428571\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.8\r\n",
    "\r\n",
    "The vectors representing the presence and absence of words in both `Y` and `Z` are very similar (Jaccard = 0.85), despite major differences at the DNA level between these two sequences. This is because the words are small -- it is as if you were comparing a history book with a book on Python using words of size 2. It's very likely that both books will contain the same words of size 2. Increasing the size of `k` will produce substantial differences. \r\n",
    "\r\n",
    "Change the function `get_kmer_2` so that given a sequence `X` and a k-mer size `k`, the function returns a boolean vector of all the words of size `k` in `X`. Cal the function `get_kmer`\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "The following code can be used to generate all DNA words of size `k`\r\n",
    "```pyton\r\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "```\r\n",
    "\r\n",
    "Once done, use the code below to test your function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Write you code here\r\n",
    "def get_kmer(X, k):\r\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "    words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\r\n",
    "    words_in_X = set([X[i:i+k] for i in range(0, len(X)-k+1)])\r\n",
    "    for word in list(words_in_X):\r\n",
    "        counts[word] = 1\r\n",
    "    return counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\r\n",
    "X = \"ACGTGATGATTG\"\r\n",
    "\r\n",
    "counts = get_kmer(X, k=1)\r\n",
    "assert counts.tolist() == [1,1,1,1]\r\n",
    "\r\n",
    "\r\n",
    "counts = get_kmer(X, k=3)\r\n",
    "assert (counts[[\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]] == 1).sum()  == 8\r\n",
    "assert (counts.drop([\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]) == 0).sum()  == 56"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.9\n",
    "\n",
    "* Compute the Jaccard similarity for the pairs `(X, Y)`, `(X, Z)`, `(Y, Z)` using `k= 5`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Write you code here\r\n",
    "X = \"ACGTACGTACGTACGT\"\r\n",
    "Y = \"ACGTACAAACGTACGT\"\r\n",
    "Z = \"TTTTACAAACGTTTTT\"\r\n",
    "\r\n",
    "print('Jaccard similarity between X and Y: ', jaccard(get_kmer(X, 5),get_kmer(Y, 5)))\r\n",
    "print('Jaccard similarity between X and Z: ', jaccard(get_kmer(X, 5),get_kmer(Z, 5)))\r\n",
    "print('Jaccard similarity between Y and Z: ', jaccard(get_kmer(Y, 5),get_kmer(Z, 5)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jaccard similarity between X and Y:  0.4\n",
      "Jaccard similarity between X and Z:  0.0\n",
      "Jaccard similarity between Y and Z:  0.29411764705882354\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The appropriate word size varies with the length of the text, with larger words depicting similarity more accurately. However, large values of `k` are:\n",
    "1. More computationally intensive to compute. With k = 12, there are $4^12 = 16,777,216$ words to compute for each sequence.\n",
    "\n",
    "2. More likely to skew the distance between fairly similar sequences. For example `k=8`, the Jaccard index between `X` and `Y` is `0`, even though `X` and `Y` have only two mismatching characters. While this is an extreme case due to the fact that X and Y are short, the logic applies to longer sequences and larger values of `k`\n",
    "\n",
    "\n",
    "![](https://www.dropbox.com/s/rhw5szbiohsqu7w/mismatches.png?dl=1)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Write you code here\r\n",
    "# the Jaccard index between X and Y is not 0, since both share the 8-letter word ACGTACGT\r\n",
    "print('Jaccard similarity between X and Y: ', jaccard(get_kmer(X, 8),get_kmer(Y, 8)))\r\n",
    "print('Jaccard similarity between X and Z: ', jaccard(get_kmer(X, 8),get_kmer(Z, 8)))\r\n",
    "print('Jaccard similarity between Y and Z: ', jaccard(get_kmer(Y, 8),get_kmer(Z, 8)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jaccard similarity between X and Y:  0.08333333333333333\n",
      "Jaccard similarity between X and Z:  0.0\n",
      "Jaccard similarity between Y and Z:  0.125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code I used is provided as a reference below. The code took 7 hours to complete on a single machine and approximately 12 minutes on a larger server with 72 cores and 1TB of RAM. To parallelize the execution, I split the file into files that contain 1000 sequences each and used GNU Parallel to run each file on a single CPU core."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# CODE PROVIDED FOR ILLUTRATION -- DO NOT REMOVE\r\n",
    "# RUNNING LOCALLY MAY TAKE A LONG TIME TO COMPLETE\r\n",
    "\r\n",
    "# k = 8 \r\n",
    "# DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "# words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "\r\n",
    "    \r\n",
    "# def get_kmer_mod(X):\r\n",
    "#     counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\r\n",
    "#     words_in_X = set([X[i:i+k] for i in range(0, len(X)-k+1)])\r\n",
    "#     counts[list(words_in_X)] = 1\r\n",
    "#     return counts   \r\n",
    "\r\n",
    "# def replace_bad_nucs(seq):\r\n",
    "#     for character in ['W', 'K', \"Y\", \"M\", 'H']:\r\n",
    "#         seq = seq.replace(character, 'A') \r\n",
    "        \r\n",
    "#     for character in ['R', 'S', 'D', \"V\", \"B\"]:\r\n",
    "#         seq = seq.replace(character, 'C') \r\n",
    "        \r\n",
    "#     seq = seq.replace(\"N\", '') \r\n",
    "    \r\n",
    "#     return seq\r\n",
    "\r\n",
    "# all_counts = []\r\n",
    "# all_names = []\r\n",
    "# with tqdm(total=1000) as pbar:\r\n",
    "#     for record in SeqIO.parse(\"myseq0.fa\", 'fasta'):\r\n",
    "#         all_names.append(record.id)\r\n",
    "#         seq = replace_bad_nucs(str(record.seq))\r\n",
    "\r\n",
    "#         counts = get_kmer_mod(seq)\r\n",
    "#         all_counts.append(counts)\r\n",
    "#         pbar.update(1)\r\n",
    "    \r\n",
    "# kmer_counts = pd.DataFrame(all_counts, index = all_names)\r\n",
    "# kmer_counts.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hashing Sequences\n",
    "\n",
    "We are interested in finding pairs of sequences that are very similar. However, comparing the sequences pairwise is not tractable since it would require carrying out $429282 * (429282 - 1) / 2 = 92_141_303_121$ comparisons.\n",
    "\n",
    "Instead, we will use the hashing-based approach covered in class. Rather than hashing a sequence over all k-mers, we will only compute the hash for a subset of k-mers. we will repeat the operation n times to avoid that similar sequences are assigned to different bins due to a single, rare mismatch.\n",
    "\n",
    "This, as discussed in class, is computationally more efficient compared to computing all pairwise sequences. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.10 \r\n",
    "\r\n",
    "Write a function that takes a `pandas  Series` and a subset of columns and returns the hash computed on the subset of columns. Call this function`hash_on_subset`.\r\n",
    "\r\n",
    "As an example, consider all words with a size of 2 as follows \r\n",
    "\r\n",
    "|\t|AA\t|AC\t|AG\t|AT\t|CA | CC| CG| CT| GA| GC| GG| GT| TA| TC| TG| TT|\r\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\r\n",
    "| A\t|0\t|1\t|0\t|0\t|0\t|0\t|1\t|0\t| 0 |0\t|0\t| 1 |1  |0\t|0\t|0  |\r\n",
    "\r\n",
    "```python\r\n",
    "hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) \r\n",
    "```\r\n",
    "\r\n",
    "is equivalent to:\r\n",
    "\r\n",
    "```python\r\n",
    "hash((1, 1, 0, 1, 1)) == 5085477689562523216\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# Write you code here\r\n",
    "def hash_on_subset(A, cols):\r\n",
    "    col_vector = A[cols]\r\n",
    "    return hash(tuple(col_vector.tolist()))\r\n",
    "\r\n",
    "hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5085477689562523216"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The method `sample` from the random module `m` words from a list\n",
    "\n",
    "For example, running:\n",
    "```python\n",
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )\n",
    "```\n",
    "returns\n",
    "```\n",
    "['A', 'C']\n",
    "```\n",
    "The returned subset may be different for you.\n",
    "\n",
    "* The code below randomly selects `m=20` k-mers we will use to compare the genomes\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['A', 'G']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "k=8\r\n",
    "DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "\r\n",
    "m=20\r\n",
    "subset_kmers = random.sample(words_size_k, m)\r\n",
    "\r\n",
    "# subset_kmers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.12\r\n",
    "\r\n",
    "\r\n",
    "Apply the function `hash_subset` to all the rows of `all_kmers_df`. The data science (*vectorized*) way to do so is using the `apply` method available on a `pandas DataFrame` instead of using for loops. For example, given a DataFrame `df` such that:\r\n",
    "\r\n",
    "```\r\n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\r\n",
    "\r\n",
    "```\r\n",
    "then \r\n",
    "```\r\n",
    "df.apply(max, args=[] axis=1)\r\n",
    "```\r\n",
    "applies the `max()` function on each row (`axis = 1`). Here, `args` is empty since `max` does not take any additional arguments.\r\n",
    "\r\n",
    "The example below shows how to use `apply` when the function requires additional arguments. In this example, we apply a function that sums all the values of a row and adds to the sum an offset (2 by default)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# EXAMPLE CODE PROVIDED -- DO NOT REMOVE\r\n",
    "def add_val_to_sum(x, offset=2):\r\n",
    "    return x.sum() + offset\r\n",
    "    \r\n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\r\n",
    "\r\n",
    "print(\"The sum of rows + an offset of 5 is:\")\r\n",
    "print(df.apply(add_val_to_sum, args=[5], axis=1))\r\n",
    "\r\n",
    "print(\"The sum of rows + an offset of 10 is:\")\r\n",
    "print(df.apply(add_val_to_sum, args=[10], axis=1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The sum of rows + an offset of 5 is:\n",
      "0    11\n",
      "1    20\n",
      "dtype: int64\n",
      "The sum of rows + an offset of 10 is:\n",
      "0    16\n",
      "1    25\n",
      "dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q.13\r\n",
    "\r\n",
    "\r\n",
    "Use `apply()` to apply `hash_subset` and compute the hash values for all the rows of `all_kmers_df` over `subset_kmers`\r\n",
    "\r\n",
    "* Create a dict by parsing the results to group sequences that yield the same hash under the same bins. Each key in the dict should be a key and each value is a list of sequences that have the same value.\r\n",
    "\r\n",
    "For example, in the dictionary below, X and Y have the same hash value (123456) over a given subset of kmers, whereas Z has a different hash over the same subsets.\r\n",
    "\r\n",
    "```\r\n",
    "{\"123456\": [X,Y], \"654321\": [Z]}\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Write your code here\r\n",
    "\r\n",
    "# initialize the dictionary\r\n",
    "kmers_dic = {}\r\n",
    "\r\n",
    "# get the column names\r\n",
    "with open('data/all_kmers_10kaa', 'r') as f:\r\n",
    "    header = next(f)\r\n",
    "    header = header[1:].split(',')\r\n",
    "    header.insert(0, 'name')\r\n",
    "f.close()\r\n",
    "\r\n",
    "# all_kmers_10k.csv was split into 16 files, each with <= 750 lines\r\n",
    "# for each file, convert it to a dataframe, use apply to hash, and record the hashes in the dict\r\n",
    "suffixes = 'abcdefghijklmn'\r\n",
    "for suffix in suffixes:\r\n",
    "    df = pd.read_csv('data/all_kmers_10ka'+suffix, header=0, names=header)\r\n",
    "    df['hash'] = df.apply(hash_on_subset, args=[subset_kmers], axis=1)\r\n",
    "    for kmer_hash in df['hash'].unique():\r\n",
    "        if kmer_hash in kmers_dic.keys():\r\n",
    "            kmers_dic[kmer_hash].append(df[df['hash'] == kmer_hash]['name'].tolist())\r\n",
    "        else:\r\n",
    "            kmers_dic[kmer_hash] = df[df['hash'] == kmer_hash]['name'].tolist()\r\n",
    "\r\n",
    "print(kmers_dic)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{6076106645815868846: ['MZ512786.1', 'MW901853.1', 'OU594424.1', 'OU393733.1', 'OB984594.1', 'MZ083704.1', 'MT534304.1', 'OU594356.1', 'OU593637.1', 'OU562833.1', 'OU534232.1', 'OU532971.1', 'OU491573.1', 'OU490803.1', 'OU490035.1', 'OU489269.1', 'OU488497.1', 'OU487720.1', 'OU475626.1', 'OU474825.1', 'OU471793.1', 'OU448300.1', 'OU447580.1', 'OU423463.1', 'OU421953.1', 'OU420295.1', 'OU418240.1', 'OU394878.1', 'OU392284.1', 'OU389525.1', 'OU386650.1', 'OU348295.1', 'MZ185747.1', 'MZ284028.1', 'OU489389.1', 'MZ257336.1', 'OU593680.1', 'MZ294678.1', 'OU392602.1', 'OU012726.1', 'OD982414.1', 'OD974555.1', 'OD960761.1', 'OD946256.1', 'OD935174.1', 'OD922364.1', 'MZ075177.1', 'OU443296.1', 'OU420627.1', 'OU396943.1', 'OU394724.1', 'OU392531.1', 'OU390249.1', 'OU387782.1', 'OU385480.1', 'OA998179.1', 'MW831547.1', 'OU421591.1', 'OU395999.1', 'OU391982.1', 'OU389998.1', 'OU385383.1', 'OU050891.1', 'OU031815.1', 'OU030894.1', 'OU029842.1', 'OU013726.1', 'OU013128.1', 'OU012505.1', 'OD997057.1', 'OD996458.1', 'OD995846.1', 'OD995267.1', 'OD994634.1', 'OD994055.1', 'OD990980.1', 'OD990409.1', 'OD989831.1', 'OD989253.1', 'OD988669.1', 'OD988077.1', 'OD986040.1', 'OD985444.1', 'OD984861.1', 'OD984272.1', 'OD983652.1', 'OD983036.1', 'OD982296.1', 'OD981549.1', 'OD980971.1', 'OD980408.1', 'OD979805.1', 'OD979098.1', 'OD978473.1', 'OD977797.1', 'OD977012.1', 'OD976273.1', 'OD975482.1', 'OD974815.1', 'OD974071.1', 'OD973421.1', 'OD972875.1', 'OD972296.1', 'OD971726.1', 'OD971142.1', 'OD970547.1', 'OD969963.1', 'OD969410.1', 'OD968795.1', 'OD968146.1', 'OD967517.1', 'OD966925.1', 'OD966291.1', 'OD965727.1', 'OD965159.1', 'OD964586.1', 'OD964035.1', 'OD963394.1', 'OD962715.1', 'OD961954.1', 'OD961303.1', 'OD960685.1', 'OD960049.1', 'OD959379.1', 'OD955333.1', 'OD954769.1', 'OD954193.1', 'OD953581.1', 'OD952894.1', 'OD952223.1', 'OD951524.1', 'OD950834.1', 'OD950148.1', 'OD948786.1', 'OD948200.1', 'OD947518.1', 'OD946811.1', 'OD945957.1', 'OD945224.1', 'OD944510.1', 'OD943669.1', 'OD942969.1', 'OD942281.1', 'OD941647.1', 'OD941000.1', 'OD940243.1', 'OD939499.1', 'OD938805.1', 'OD938204.1', 'OD937549.1', 'OD936806.1', 'OD935139.1', 'OD933728.1', 'OD933008.1', 'OD932299.1', 'OD931689.1', 'OD931093.1', 'OD930294.1', 'OD929499.1', 'OD928779.1', 'OD928165.1', 'OD927480.1', 'OD926819.1', 'OD926206.1', 'OD924962.1', 'OD924151.1', 'OD923358.1', 'OD922478.1', 'OD921844.1', 'OD920464.1', 'OD919500.1', 'OD918721.1', 'OD917956.1', 'OD916945.1', 'OD915640.1', 'OD914915.1', 'OD913849.1', 'OD912585.1', 'OD911523.1', 'OD910162.1', 'OD907162.1', 'OD906396.1', 'OD904630.1', 'OD900608.1', 'OB998533.1', 'OB993305.1', 'MZ578700.1', 'MZ283459.1', 'MZ212871.1', 'MZ156650.1', 'MZ022099.1', 'MW868975.1'], 1456255203632785470: ['MZ211102.1', 'OU419015.1'], -3290054127330540942: ['OU593270.1', 'OU488506.1'], 6284508153676357819: ['OU393921.1'], 5485051165973601667: ['OU387600.1'], -3774042006148519206: ['OD934415.1']}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q. 15\n",
    "\n",
    "Here we use the presence and absence of words, i.e., a vector of booleans, to encode a sequence. The problem with this approach is that it considers the sequences to be identical, even if their word counts differ substantially. For example, given the sequence `X`, `Y` and `Z` as follows\n",
    "```\n",
    "X = ATAGATAGATAGATAGATT\n",
    "Y = ATAGATAGATAGATAGATT\n",
    "Z = ATAGATTTTTTTTTTTTTT\n",
    "```\n",
    "With k =2, all three sequences mach on their vector of word presence/absense."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "X = \"ATAGATAGATAGATAGATT\"\r\n",
    "Y = \"ATAGATAGATAGATAGATT\"\r\n",
    "Z = \"ATAGATTTTTTTTTTTTTT\"\r\n",
    "\r\n",
    "counts_dict = {\"X\": get_kmer(X, k=2), \"Y\": get_kmer(Y, k=2), \"Z\": get_kmer(Z, k=2)}\r\n",
    "pretty_print_counts(counts_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1\n",
       "Y   1   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1\n",
       "Z   0   0   1   1   0   0   0   0   1   0   0   0   1   0   0   1"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Comparing these sequences based on word counts, X is much more similar to Y than it is to Z"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def get_kmer_counts(X, k):\r\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "    words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\r\n",
    "    counts_words_in_x = Counter([X[i:i+k] for i in range(0, len(X)-k+1)])\r\n",
    "    counts.update(counts_words_in_x)\r\n",
    "    return counts    \r\n",
    "\r\n",
    "\r\n",
    "counts_dict = {\"X\": get_kmer_counts(X, k=2), \"Y\": get_kmer_counts(Y, k=2), \"Z\": get_kmer_counts(Z, k=2)}\r\n",
    "pretty_print_counts(counts_dict)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AA</th>\n",
       "      <th>AC</th>\n",
       "      <th>AG</th>\n",
       "      <th>AT</th>\n",
       "      <th>CA</th>\n",
       "      <th>CC</th>\n",
       "      <th>CG</th>\n",
       "      <th>CT</th>\n",
       "      <th>GA</th>\n",
       "      <th>GC</th>\n",
       "      <th>GG</th>\n",
       "      <th>GT</th>\n",
       "      <th>TA</th>\n",
       "      <th>TC</th>\n",
       "      <th>TG</th>\n",
       "      <th>TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT\n",
       "X   0   0   4   5   0   0   0   0   4   0   0   0   4   0   0   1\n",
       "Y   0   0   4   5   0   0   0   0   4   0   0   0   4   0   0   1\n",
       "Z   0   0   1   2   0   0   0   0   1   0   0   0   1   0   0  13"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Given an example (vectors) to justify why hashing is not ideal with counts.\n",
    "  * Use any means you think are useful to illustrate your point (e.g.: figure, simulation (yes, please!))\n",
    " \n",
    "* Describe how the random project approach discussed in class can help solve the issue discussed\n",
    "  * Use code to illustrate how random projection works in the following example.\n",
    "    * I.e., provide code to provide an example where `X` and `Y` are assigned to the same bin and `Y` is assigned to a different bin.\n",
    "    * You can choose any vector values as needed \n",
    " \n",
    "```python\n",
    "X = [1,2]\n",
    "Y = [2,2]\n",
    "Z = [5,1]\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "### Write your code here\r\n",
    "\r\n",
    "# This simulation considers subwords of different lengths of X and Y\r\n",
    "# For each length k, \r\n",
    "    # the simulation gets the number of occurences of subwords of length k, \r\n",
    "    # calculates the sum of the absolute value of the differences of the subword count for each subword,\r\n",
    "    # and prints the hash of the subword count\r\n",
    "\r\n",
    "# Since X and Y are similar, their subword counts are consistently similar (although the subword difference increases as subword length increases)\r\n",
    "# However, the hashes of the counts are consistently different, showing that hashing counts does not preserve much information\r\n",
    "\r\n",
    "for k in range(2, 8):\r\n",
    "    print('Considering subwords of length ', k)\r\n",
    "    counts_dict = {\"X\": get_kmer_counts(X, k), \"Y\": get_kmer_counts(Y, k)}\r\n",
    "    counts_df = pd.DataFrame(counts_dict)\r\n",
    "    print('The total difference between the word counts of X and Y is ', sum(abs(counts_df['X'] - counts_df['Y'])))\r\n",
    "    print('Hash of counts of X: ', hash(tuple(counts_df['X'])))\r\n",
    "    print('Hash of counts of Y: ', hash(tuple(counts_df['Y'])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Considering subwords of length  2\n",
      "The total difference between the word counts of X and Y is  4\n",
      "Hash of counts of X:  -5501209074414000334\n",
      "Hash of counts of Y:  -4569096340364455988\n",
      "Considering subwords of length  3\n",
      "The total difference between the word counts of X and Y is  6\n",
      "Hash of counts of X:  3452679552219961735\n",
      "Hash of counts of Y:  2103256299512720075\n",
      "Considering subwords of length  4\n",
      "The total difference between the word counts of X and Y is  8\n",
      "Hash of counts of X:  -180214422177686404\n",
      "Hash of counts of Y:  8731928759466027529\n",
      "Considering subwords of length  5\n",
      "The total difference between the word counts of X and Y is  10\n",
      "Hash of counts of X:  2613188667411611157\n",
      "Hash of counts of Y:  1358985143806817381\n",
      "Considering subwords of length  6\n",
      "The total difference between the word counts of X and Y is  12\n",
      "Hash of counts of X:  7563226335486792872\n",
      "Hash of counts of Y:  -3193794736243872645\n",
      "Considering subwords of length  7\n",
      "The total difference between the word counts of X and Y is  14\n",
      "Hash of counts of X:  5045137861792538914\n",
      "Hash of counts of Y:  -3828125291708460112\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}